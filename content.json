{"meta":{"title":"Blog","subtitle":"Blog","description":"略懂设计的程序员","author":"佐伯楽","url":"https://saekiraku.github.io"},"pages":[{"title":"关于我","date":"2018-05-17T05:58:23.000Z","updated":"2018-05-17T05:59:52.883Z","comments":true,"path":"about/index.html","permalink":"https://saekiraku.github.io/about/index.html","excerpt":"","text":"I’m nobody"}],"posts":[{"title":"Webpack 前端编译策略的优化","slug":"Webpack编译策略与优化","date":"2018-05-17T06:01:55.000Z","updated":"2018-05-17T09:50:06.446Z","comments":true,"path":"2018/05/17/Webpack编译策略与优化/","link":"","permalink":"https://saekiraku.github.io/2018/05/17/Webpack编译策略与优化/","excerpt":"在前端工程化盛行的今天，很多公司为了提升开发效率与项目可维护性，都开始使用上了webpack这样的自动化工具。在享受自动化高效率的同时，也产生了一些性能上的问题：代码重复引用、臃肿的混合JS库、编译速度慢……，很多技术人员面对此类问题无从下手，求助于百度但是却不求甚解。所以本文将通过问题、分析、原因、解决方案几个步骤，来讲解前端自动化中的优化问题。","text":"在前端工程化盛行的今天，很多公司为了提升开发效率与项目可维护性，都开始使用上了webpack这样的自动化工具。在享受自动化高效率的同时，也产生了一些性能上的问题：代码重复引用、臃肿的混合JS库、编译速度慢……，很多技术人员面对此类问题无从下手，求助于百度但是却不求甚解。所以本文将通过问题、分析、原因、解决方案几个步骤，来讲解前端自动化中的优化问题。 注意：本文假设您已有webpack基础，并了解基本的配置项的意义。在此基础上希望能了解一些webpack的优化规则。 问题1：编译后文件体积过大该问题是指执行编译后，webpack在控制台输出的数据中的资源体积过大。一般会用黄色的字体标注上[big]。有时编译策略的错误，会导致代码文件达到5、6MB甚至10MB以上。假设用户的网速是2MB/s，加载一次页面也需要3s，并且再考虑到服务器的出口带宽，可能就需要数十秒才能显示出页面。 但是，文件体积过大又分很多种情况，所以要具体情况具体分析。我们先从最简单的开始说。 CSS文件过大一般CSS文件不会过大，而且也没什么可优化的，毕竟CSS只是描述样式，没有逻辑，所以不会像JS出现重复引用的问题。只需要确保 LoaderOptionsPlugin 下开启了 minimize 。打包后的CSS就会自动去除不必要的换行符和空格等。 JS文件过大JS 文件过大的情况处理起来就比较复杂了，因为有很多可能性会导致JS文件过大。为了不盲目的解决问题，我们需要使用 webpack-bundle-analyzer 这个工具来分析具体是什么原因导致的问题。 重复引用重复引用是一个非常常见的问题，虽然很多 webpack 脚手架都自带了 CommonsChunkPlugin 的配置 —— 将重复引用的JS模块打包进一个总的混合JS包中(常见命名：Common.js / Vendor.js / Lib.js……)，这样做的好处是降低页面加载时的并发量。参见：加载缓慢/HTTP并发 但是偶尔的，在某些特定情况下 CommonsChunkPlugin 反而会产生相反的作用。我们使用 webpack-bundle-analyzer 分析一下编译后的代码引用结构。 仔细观察上图，我们就会发现所有被按需加载的页面，都反复加载了 Ant.Design 组件库中使用到的组件。假如每个页面都用到了 Button 组件，那么当用户浏览网页中的其他内容时， Button 组件将会反复的被加载，白白浪费了网络带宽。 这个问题是因为项目按照 Ant.Design 官方文档的配置，增加了组件库的按需加载。因此使整个组件库独立出 CommonsChunkPlugin 的策略外了。 所以针对这个特殊情况，我们其实应该将 Ant.Design 从代码中抽离出来，然后使用 CDN 加载。虽然一次性的加载了一个非常大的类库，但是进入缓存后，后面的加载就会非常快了。同时，由于是后台系统，基本上一个页面中能用到的组件都用到了，使用按需加载会反复加载N次很多相同的组件代码。两者相比较起来，按需加载实际上就没有意义了。 第三方类库第三方类库一般是引起 Vendor.js 这样的文件过大的主要原因，实际上除非特殊需要，我们没有必要将第三方类库打包进我们的项目中。一般都是通过一些第三方的CDN服务(如：BootCDN)来引入类库，1是能减少网站自身的流量使用，2是用户访问过其他网站，并且那个网站使用了同一个CDN地址时，就会直接命中缓存，而不用重新加载，减少了用户的等待时间。 webpack版本过低这个问题目前（2018-05-17）应该不会有了，毕竟都已经webpack 4了。这里主要是想提一句 webpack 2 时增加的 Tree Shaking 代码优化技术。在一些公司的面试中可能会有考核，更多的内容，自己百度一下把。 媒体资源过大这个问题其实不太属于前端的范畴，如果你们公司的UI比较负责的话，在给你媒体资源之前应该就已经优化过了。 问题2：加载缓慢当我们通过问题1缩减 加载请求并发假设我们一个页面引用了 10 余个JS模块，当加载这些JS文件时，HTTP协议会进行3次握手4次挥手，相当于加载完整个页面，就与服务器进行了70余次的连接/断开通信，大部分时间就都浪费在了建立连接的网络通信上。因此我们使用 CommonsChunkPlugin 将各个页面都会用到的模块打包进一个综合的Vendor.js中。来降低多次建立连接与并发。 启用缓存通过上面的步骤尽可能的优化代码体积后，下一步要做的就是开启缓存。如果不开启缓存的话，用户每一次打开网站，都会从服务器重复的加载相同的资源文件。这样显然也是白白浪费带宽了。 但是开启缓存后，如果我们更新了网站代码，浏览器会由于缓存原因，无法立刻同步最新代码。因此，我们需要使用 webpack 的 hash 标签来为资源文件进行命名。这样每一次编译的时候，资源文件都会被命名为类似 xxxx.fea39cda7bxxxx.js ，并且当文件内容发生变化后，hash 值也会发生变化，这样就会导致浏览器重新加载这个文件。也就达到了更新版本的目的了。 但是别忘了不要给html加缓存，否则及时更新仍然是无效的。以Nginx为例的配置： 123456789location ~ html$ &#123; ... add_header Cache-Control no-store;&#125;location ~ (js|css|png|jpg)$ &#123; ... expires 30d;&#125; 问题3：编译缓慢","categories":[{"name":"前端","slug":"前端","permalink":"https://saekiraku.github.io/categories/前端/"}],"tags":[{"name":"webpack","slug":"webpack","permalink":"https://saekiraku.github.io/tags/webpack/"}]}]}